{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxrELqym3uJo"
      },
      "source": [
        "# Multilingual Retrieval-Augmented Generation (RAG) System\n",
        "\n",
        "Welcome! This notebook implements a simple RAG pipeline capable of answering queries in both Bangla and English by retrieving relevant information from a PDF corpus and generating grounded answers.\n",
        "\n",
        "---\n",
        "\n",
        "### How to Use This Notebook\n",
        "\n",
        "1. **Run all cells sequentially from top to bottom** — this will install dependencies, load and process the PDF, build the knowledge base, start the API server, and set up the chat client.\n",
        "\n",
        "2. After running all setup cells, **use the last cell to interact with the chat client.**  \n",
        "   You can ask questions in Bangla or English and receive answers grounded in the document.\n",
        "\n",
        "---\n",
        "\n",
        "> Make sure to follow the instructions in each cell carefully. If you upload a new PDF, rerun the knowledge base build cell.\n",
        "\n",
        ">And if just willing to evaluate, just watch the output of cells.\n",
        "\n",
        "Enjoy exploring the RAG system!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RrilHQNzvne"
      },
      "source": [
        "**Required packages to install**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lIlqlC1DpFiS",
        "outputId": "6a8a1afc-ab3c-4b7f-97d1-cf5dd0f61e15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.7)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (4.1.0)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: groq in /usr/local/lib/python3.11/dist-packages (0.30.0)\n",
            "Requirement already satisfied: pdfminer.six==20250506 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20250506)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.3.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.0)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (3.4.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250506->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.16.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.33.4)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.14.1)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.7)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.4.1)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.35.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.0.2)\n",
            "Requirement already satisfied: posthog<6.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.22.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.35.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.35.0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.35.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.2)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.73.1)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (33.1.0)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (8.5.0)\n",
            "Requirement already satisfied: pyyaml>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.11.0)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from groq) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.26.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.3.1)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.5.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.57 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.35.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.35.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.35.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.35.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.56b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.56b0)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog<6.0.0,>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.1)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.22)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.11/dist-packages (0.116.1)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.11/dist-packages (7.2.12)\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.11/dist-packages (0.35.0)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.11/dist-packages (0.0.20)\n",
            "Requirement already satisfied: starlette<0.48.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (0.47.2)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from fastapi) (2.11.7)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from fastapi) (4.14.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (8.2.1)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.11/dist-packages (from uvicorn) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,!=2.0.0,!=2.0.1,!=2.1.0,<3.0.0,>=1.7.4->fastapi) (0.4.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.11/dist-packages (from starlette<0.48.0,>=0.40.0->fastapi) (4.9.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.6.2->starlette<0.48.0,>=0.40.0->fastapi) (1.3.1)\n",
            "Requirement already satisfied: pytesseract in /usr/local/lib/python3.11/dist-packages (0.3.13)\n",
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.11/dist-packages (1.17.0)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (25.0)\n",
            "Requirement already satisfied: Pillow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from pytesseract) (11.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pdfplumber sentence-transformers chromadb transformers accelerate groq\n",
        "!pip install fastapi nest-asyncio pyngrok uvicorn python-multipart\n",
        "!pip install pytesseract pdf2image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OREoV2QJNGnD",
        "outputId": "ce6fc9c3-aa42-46a2-8868-1f8910aa9f4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr is already the newest version (4.1.1-2.1build1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "tesseract-ocr-ben is already the newest version (1:4.00~git30-7274cfa-1.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "poppler-utils is already the newest version (22.02.0-2ubuntu0.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install tesseract-ocr -y\n",
        "!apt-get install tesseract-ocr-ben -y\n",
        "!apt-get install poppler-utils -y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C00KlKD9ske4",
        "outputId": "9afe84a1-44d5-4652-bbc9-7aaa3ac07956"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok authtoken 30P4RXpkpS0fudAyUOlySNPpqDg_FRTvxRNDye7FsfUeACmP"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTMtknzC0DAP"
      },
      "source": [
        "**Necessary imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TqBjHRxgpFhY"
      },
      "outputs": [],
      "source": [
        "import pdfplumber\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import chromadb\n",
        "from transformers import pipeline\n",
        "import torch\n",
        "import shutil\n",
        "import os\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "from groq import Groq\n",
        "from typing import List\n",
        "from google.colab import files\n",
        "from fastapi import FastAPI, Request, HTTPException\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from pydantic import BaseModel\n",
        "from collections import defaultdict, deque\n",
        "import uvicorn\n",
        "from pyngrok import ngrok\n",
        "import nest_asyncio\n",
        "import threading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1keaCuAv06FQ"
      },
      "source": [
        "### Text Extraction and Cleaning\n",
        "\n",
        "This cell defines two important functions for processing the PDF documents:\n",
        "\n",
        "- **`extract_text_from_pdf(pdf_path)`**:  \n",
        "  Converts each page of the PDF into an image and then uses Tesseract OCR to extract Bengali text from those images. This is useful especially for scanned PDFs or PDFs without selectable text.\n",
        "\n",
        "- **`clean_text(text)`**:  \n",
        "  Cleans the raw extracted text by removing unwanted patterns such as page headers, question numbering, multiple-choice options, answer hints, and non-Bangla characters. This cleaning helps improve the quality of the data before chunking and embedding for retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "F1p0pNyfJett"
      },
      "outputs": [],
      "source": [
        "def extract_text_from_pdf(pdf_path):\n",
        "    images = convert_from_path(pdf_path)\n",
        "\n",
        "    texts = []\n",
        "    for i, img in enumerate(images):\n",
        "        text = pytesseract.image_to_string(img, lang='ben')\n",
        "        texts.append(text)\n",
        "\n",
        "\n",
        "    return texts\n",
        "\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "    text = re.sub(r'=+ Page \\d+ =+', '', text)\n",
        "    text = re.sub(r'\\b[০-৯]{1,3}।', '', text)\n",
        "    text = re.sub(r'[\\(\\[]?[কখগঘ][)\\].]', '', text)\n",
        "    text = re.sub(r'[কখগঘ][)\\.:\\s]', '', text)\n",
        "    text = re.sub(r'উ[ত্তরঃ:\\s]*[কখগঘ]', '', text)\n",
        "    text = re.sub(r'^\\s*[কখগঘ][)\\.]', '', text, flags=re.MULTILINE)\n",
        "    text = re.sub(r'[^\\u0980-\\u09FF০-৯\\s.,!?;:\\-]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuRbcfI31I1n"
      },
      "source": [
        "### Paragraph-Based Chunking\n",
        "\n",
        "Splits text into chunks of up to 200 words by combining paragraphs.  \n",
        "Useful for preparing text for embedding and retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pysl57UYpDPn"
      },
      "outputs": [],
      "source": [
        "def chunk_by_paragraph(text, max_words=200):\n",
        "    paragraphs = re.split(r'\\n\\s*\\n', text)\n",
        "\n",
        "    chunks = []\n",
        "    current_chunk = \"\"\n",
        "\n",
        "    for para in paragraphs:\n",
        "        para = para.strip()\n",
        "        if not para:\n",
        "            continue\n",
        "\n",
        "        if len(current_chunk.split()) + len(para.split()) <= max_words:\n",
        "            current_chunk += \" \" + para\n",
        "        else:\n",
        "            if current_chunk:\n",
        "                chunks.append(current_chunk.strip())\n",
        "            current_chunk = para\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(current_chunk.strip())\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAYQfqjD1PwP"
      },
      "source": [
        "### Initialize Embedding Model and Vector Store\n",
        "\n",
        "- Load the multilingual SentenceTransformer model (`intfloat/multilingual-e5-base`) for text embeddings.\n",
        "- Set up a Chroma vector database client.\n",
        "- Delete any existing collection named `pdf_collection`.\n",
        "- Create a new collection to store document embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y52kSyf1pDS_",
        "outputId": "0c6a5de7-f104-4407-c510-86eb6c812755"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "embed_model_name = \"intfloat/multilingual-e5-base\"\n",
        "embed_model = SentenceTransformer(embed_model_name)\n",
        "\n",
        "client = chromadb.Client()\n",
        "collection_name = \"pdf_collection\"\n",
        "\n",
        "if collection_name in [col.name for col in client.list_collections()]:\n",
        "    client.delete_collection(collection_name)\n",
        "\n",
        "collection = client.create_collection(name=collection_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py4pqR7_1fhX"
      },
      "source": [
        "### Upload PDF and Build Knowledge Base\n",
        "\n",
        "- Upload your PDF file by running the next code cell.  \n",
        "- A file picker will appear; select the PDF document to upload (e.g., `HSC26-Bangla1st-Paper.pdf`).  \n",
        "- The uploaded file is automatically saved in the `/content` directory in Colab.  \n",
        "\n",
        "This code will:  \n",
        "- Extract text from the uploaded PDF using OCR,  \n",
        "- Clean and chunk the extracted text into manageable paragraphs,  \n",
        "- Generate embeddings for each chunk using the SentenceTransformer model,  \n",
        "- Store the embeddings and chunks in the Chroma vector database to create the knowledge base.  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "id": "QAuhcdtNpDWI",
        "outputId": "0eacb48a-802c-4798-a00e-40e9479ce51f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6eb92640-3d58-4683-818a-e6035cb3a831\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6eb92640-3d58-4683-818a-e6035cb3a831\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving HSC26-Bangla1st-Paper.pdf to HSC26-Bangla1st-Paper.pdf\n",
            "Extracted 49 pages\n",
            "Created 49 chunks\n",
            "Knowledge base built and stored in Chroma.\n"
          ]
        }
      ],
      "source": [
        "uploaded = files.upload()\n",
        "pdf_path = list(uploaded.keys())[0]\n",
        "pdf_path = os.path.join(\"/content\", pdf_path)\n",
        "\n",
        "def build_knowledge_base(pdf_path):\n",
        "    pages = extract_text_from_pdf(pdf_path)\n",
        "    print(f\"Extracted {len(pages)} pages\")\n",
        "    all_chunks = []\n",
        "    for page in pages:\n",
        "        cleaned = clean_text(page)\n",
        "        chunks = chunk_by_paragraph(cleaned)\n",
        "        all_chunks.extend(chunks)\n",
        "\n",
        "    print(f\"Created {len(all_chunks)} chunks\")\n",
        "\n",
        "    embeddings = embed_model.encode(all_chunks).tolist()\n",
        "\n",
        "    collection.add(\n",
        "        documents=all_chunks,\n",
        "        embeddings=embeddings,\n",
        "        ids=[f\"chunk_{i}\" for i in range(len(all_chunks))]\n",
        "    )\n",
        "    print(\"Knowledge base built and stored in Chroma.\")\n",
        "\n",
        "build_knowledge_base(pdf_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oA7I__Sl1qIv"
      },
      "source": [
        "### Querying the Knowledge Base\n",
        "\n",
        "This function takes a user query, converts it to an embedding, and searches the vector database (`Chroma`) for the most relevant document chunks.\n",
        "\n",
        "- `top_k`: Number of top results to return.\n",
        "- `min_score`: Minimum similarity score threshold to filter out low-relevance results.\n",
        "\n",
        "It returns the filtered document chunks that have a similarity score above the threshold, ensuring relevant context is retrieved for answering the query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "M-uINt0UpDZX"
      },
      "outputs": [],
      "source": [
        "def query_knowledge_base(query, top_k=5, min_score=0.3):\n",
        "    query_emb = embed_model.encode([query]).tolist()\n",
        "\n",
        "    results = collection.query(\n",
        "        query_embeddings=query_emb,\n",
        "        n_results=top_k,\n",
        "        include=[\"documents\", \"distances\"]\n",
        "    )\n",
        "\n",
        "    documents = results.get(\"documents\", [[]])[0]\n",
        "    scores = results.get(\"distances\", [[]])[0]\n",
        "\n",
        "    filtered = [\n",
        "        doc for doc, score in zip(documents, scores)\n",
        "        if score >= min_score\n",
        "    ]\n",
        "\n",
        "    return filtered\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0QCZ2FY51x6n"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAjjRlgR1x7y"
      },
      "source": [
        "### Setting Up the LLM Client\n",
        "\n",
        "- Initialize the Groq client using your API key.\n",
        "- Specify the language model to be used for generating answers.\n",
        "\n",
        "This setup connects your code to the Groq API and selects the `llama-4-scout-17b-16e-instruct` model for multilingual question answering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSTyAM1IVOVI"
      },
      "outputs": [],
      "source": [
        "GROQ_API_KEY = \"<paste your api key here>\"\n",
        "\n",
        "client = Groq(api_key=GROQ_API_KEY)\n",
        "\n",
        "model_name = \"meta-llama/llama-4-scout-17b-16e-instruct\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCeZXLk61-EP"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JxQafvS1-FY"
      },
      "source": [
        "### Generating Answers with Short-Term Memory\n",
        "\n",
        "- Maintains a short-term conversation history of the last 3 exchanges using a `deque`.\n",
        "- Constructs a prompt combining retrieved context and recent conversation history.\n",
        "- Sends the prompt to the Groq LLM to generate an answer **in the same language as the query**.\n",
        "- If no answer is found in the context, the model replies with \"I don't know\" in the query's language.\n",
        "- Updates the short-term memory with the current query and response to maintain context across turns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pdevUXhCpDo3"
      },
      "outputs": [],
      "source": [
        "from collections import deque\n",
        "\n",
        "SHORT_TERM_MEMORY = deque(maxlen=3)\n",
        "\n",
        "def generate_answer_groq(query, retrieved_chunks):\n",
        "    context = \"\\n\".join(retrieved_chunks)\n",
        "    history_prompt = \"\"\n",
        "    if SHORT_TERM_MEMORY:\n",
        "        history_prompt = \"\\n\\nRecent Conversation History:\\n\" + \"\\n\".join(\n",
        "            [f\"User: {q}\\nAssistant: {a}\" for q, a in SHORT_TERM_MEMORY]\n",
        "        )\n",
        "    prompt = f\"\"\"Read the context below and recent conversation history to answer the question **in the same language as the question**.\n",
        "If the answer is not present, say \"I don't know\" in the query's language.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "{history_prompt}\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant that replies in the same language as the question.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.4,\n",
        "        max_tokens=1024,\n",
        "    )\n",
        "    answer = response.choices[0].message.content.strip()\n",
        "    SHORT_TERM_MEMORY.append((query, answer))\n",
        "    return answer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNSy4lQ42GIP"
      },
      "source": [
        "### Sample Queries and Answers\n",
        "\n",
        "This cell runs a set of example queries in both Bangla and English against the RAG system.  \n",
        "For each query, it retrieves relevant document chunks and generates an answer using the Groq model.  \n",
        "If no relevant context is found, it informs that no relevant information is available.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClaILz9xpDwf",
        "outputId": "43ded182-9561-4fe1-ee62-1f364ebae12e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " প্রশ্ন: অনুপমের ভাষায় সুপুরুষ কাকে বলা হয়েছে?\n",
            "উত্তর: গজাননের কার্তিকেয়কে\n",
            "\n",
            " প্রশ্ন: কাকে অনুপমের ভাগ্য দেবতা বলে উল্লেখ করা হয়েছে?\n",
            "উত্তর: মামাকে\n",
            "\n",
            " প্রশ্ন: বিয়ের সময় কল্যাণীর প্রকৃত বয়স কত ছিল?\n",
            "উত্তর: ১৬ বছর\n",
            "\n",
            " প্রশ্ন: who is the writer of the story?\n",
            "উত্তর: রবীন্দ্রনাথ ঠাকুর\n",
            "\n",
            " প্রশ্ন: when was the writer was born?\n",
            "উত্তর: মে ৭, ১৮৬১\n",
            "\n",
            " প্রশ্ন: father name of kollani?\n",
            "উত্তর: শস্তুনাথ সেন\n",
            "\n"
          ]
        }
      ],
      "source": [
        "queries = [\n",
        "   \"অনুপমের ভাষায় সুপুরুষ কাকে বলা হয়েছে?\",\n",
        "    \"কাকে অনুপমের ভাগ্য দেবতা বলে উল্লেখ করা হয়েছে?\",\n",
        "    \"বিয়ের সময় কল্যাণীর প্রকৃত বয়স কত ছিল?\",\n",
        "    \"who is the writer of the story?\",\n",
        "    \"when was the writer was born?\",\n",
        "    \"father name of kollani?\"\n",
        "]\n",
        "\n",
        "for query in queries:\n",
        "    retrieved_chunks = query_knowledge_base(query)\n",
        "    if not retrieved_chunks:\n",
        "        print(f\" Q: {query}\")\n",
        "        print(\"প্রাসঙ্গিক তথ্য পাওয়া যায়নি।\\n\")\n",
        "        continue\n",
        "\n",
        "    answer = generate_answer_groq(query, retrieved_chunks)\n",
        "    print(f\" প্রশ্ন: {query}\")\n",
        "    print(f\"উত্তর: {answer}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoWxo1fr2oJ3"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9jN6SJ22oLQ"
      },
      "source": [
        "### REST API Server for Multilingual RAG\n",
        "\n",
        "This cell sets up a FastAPI application exposing a `/ask` endpoint.  \n",
        "- Accepts POST requests with a JSON payload containing `query` (user question) and optional `session_id` (for session-based memory).  \n",
        "- Maintains short-term conversational memory (last 3 exchanges) per session.  \n",
        "- Retrieves relevant chunks from the knowledge base and generates an answer grounded on context and conversation history.  \n",
        "- Uses the Groq model for multilingual answer generation.  \n",
        "- Returns the answer along with the session history as JSON.  \n",
        "- CORS enabled to allow requests from any origin.\n",
        "\n",
        "This API enables integration with frontend clients or other applications for real-time Q&A interaction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK1bTgDwsysv",
        "outputId": "2a3c79e9-5de6-4b3d-bfa3-cf787271a57b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FastAPI application created with /ask endpoint\n"
          ]
        }
      ],
      "source": [
        "app = FastAPI(title=\"Multilingual RAG API\")\n",
        "\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=[\"*\"],\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "session_histories = defaultdict(lambda: deque(maxlen=3))\n",
        "\n",
        "class QueryRequest(BaseModel):\n",
        "    query: str\n",
        "    session_id: str = \"default\"\n",
        "\n",
        "class QueryResponse(BaseModel):\n",
        "    answer: str\n",
        "    session_id: str\n",
        "    history: list\n",
        "\n",
        "@app.post(\"/ask\", response_model=QueryResponse)\n",
        "async def ask_question(request: Request, query_req: QueryRequest):\n",
        "    try:\n",
        "        retrieved_chunks = query_knowledge_base(query_req.query)\n",
        "\n",
        "        memory = session_histories[query_req.session_id]\n",
        "\n",
        "        answer = generate_answer_with_memory(\n",
        "            query_req.query,\n",
        "            retrieved_chunks,\n",
        "            memory\n",
        "        )\n",
        "\n",
        "        memory.append((query_req.query, answer))\n",
        "\n",
        "        history = [{\"query\": q, \"answer\": a} for q, a in memory]\n",
        "\n",
        "        return QueryResponse(\n",
        "            answer=answer,\n",
        "            session_id=query_req.session_id,\n",
        "            history=history\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=500, detail=str(e))\n",
        "\n",
        "def generate_answer_with_memory(query, retrieved_chunks, memory):\n",
        "    context = \"\\n\".join(retrieved_chunks) if retrieved_chunks else \"No relevant context found.\"\n",
        "\n",
        "    history_context = \"\"\n",
        "    if memory:\n",
        "        history_context = \"\\n\\nConversation History:\\n\" + \"\\n\".join(\n",
        "            f\"User: {q}\\nAssistant: {a}\" for q, a in memory\n",
        "        )\n",
        "\n",
        "    prompt = f\"\"\"Answer the question based on the context and conversation history.\n",
        "Respond in the SAME LANGUAGE as the question. If the answer isn't in the context,\n",
        "say \"I don't know\" in the question's language.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "{history_context}\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=model_name,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful multilingual assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.4,\n",
        "        max_tokens=1024,\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "print(\"FastAPI application created with /ask endpoint\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFyjEUwJ2_Jv"
      },
      "source": [
        "### Exposing the FastAPI Server Publicly with Ngrok\n",
        "\n",
        "This cell starts the FastAPI server on port 8000 inside a background thread and exposes it publicly using Ngrok.  \n",
        "- `ngrok.connect(8000)` creates a public URL tunnel to your local server.  \n",
        "- `uvicorn.run` launches the FastAPI app.  \n",
        "- Running the server in a separate thread allows the notebook to remain interactive.  \n",
        "- The public URL is printed so you can access the API endpoint `/ask` from anywhere.\n",
        "\n",
        "Use this URL to send POST requests or interact with the RAG system via the REST API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "apFMlGMyvPLA",
        "outputId": "4210b24f-0031-4b9c-b2b2-a2b2c9808b98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Public URL: https://ea1fff4faa21.ngrok-free.app\n",
            "Server is running! Use the public URL to access the API\n",
            "API endpoint: https://ea1fff4faa21.ngrok-free.app/ask\n"
          ]
        }
      ],
      "source": [
        "public_url = ngrok.connect(8000).public_url\n",
        "print(f\"Public URL: {public_url}\")\n",
        "\n",
        "\n",
        "def run_server():\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
        "\n",
        "thread = threading.Thread(target=run_server, daemon=True)\n",
        "thread.start()\n",
        "\n",
        "print(\"Server is running! Use the public URL to access the API\")\n",
        "print(f\"API endpoint: {public_url}/ask\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEGSbJxB3O7Y"
      },
      "source": [
        "### Interactive Chat Client for the RAG API\n",
        "\n",
        "This cell provides a simple command-line chat interface that connects to the deployed FastAPI RAG system.\n",
        "\n",
        "- You enter a session ID (or press Enter for the default session).\n",
        "- The client sends your queries to the `/ask` endpoint using HTTP POST requests.\n",
        "- The assistant’s answer is printed below each query.\n",
        "- The recent conversation history (last 3 exchanges) is displayed for context.\n",
        "- Type `exit` or `quit` to end the chat session.\n",
        "\n",
        "This allows you to easily test and interact with the RAG model in a conversational manner directly from the notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHT2Nsz4vPb5",
        "outputId": "055b0feb-12db-43da-b731-2d57fa2dc1fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Chat session started: default\n",
            "Type 'exit' to quit\n",
            "\n",
            "You: অনুপমের ভাষায় সুপুরুষ কাকে বলা হয়েছে?\",\n",
            "INFO:     34.91.73.162:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "\n",
            "Assistant:\n",
            "শস্তুনাথকে\n",
            "\n",
            "Recent History:\n",
            "\n",
            "  1) Q: অনুপমের ভাষায় সুপুরুষ কাকে বলা হয়েছে?\",\n",
            "     A: শস্তুনাথকে\n",
            "\n",
            "============================================================\n",
            "\n",
            "You: কাকে অনুপমের ভাগ্য দেবতা বলে উল্লেখ করা হয়েছে?\n",
            "INFO:     34.91.73.162:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "\n",
            "Assistant:\n",
            "মামাকে\n",
            "\n",
            "Recent History:\n",
            "\n",
            "  1) Q: অনুপমের ভাষায় সুপুরুষ কাকে বলা হয়েছে?\",\n",
            "     A: শস্তুনাথকে\n",
            "\n",
            "  2) Q: কাকে অনুপমের ভাগ্য দেবতা বলে উল্লেখ করা হয়েছে?\n",
            "     A: মামাকে\n",
            "\n",
            "============================================================\n",
            "\n",
            "You: বিয়ের সময় কল্যাণীর প্রকৃত বয়স কত ছিল?\n",
            "INFO:     34.91.73.162:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "\n",
            "Assistant:\n",
            "১৬ বছর\n",
            "\n",
            "Recent History:\n",
            "\n",
            "  1) Q: অনুপমের ভাষায় সুপুরুষ কাকে বলা হয়েছে?\",\n",
            "     A: শস্তুনাথকে\n",
            "\n",
            "  2) Q: কাকে অনুপমের ভাগ্য দেবতা বলে উল্লেখ করা হয়েছে?\n",
            "     A: মামাকে\n",
            "\n",
            "  3) Q: বিয়ের সময় কল্যাণীর প্রকৃত বয়স কত ছিল?\n",
            "     A: ১৬ বছর\n",
            "\n",
            "============================================================\n",
            "\n",
            "You:  who is the writer of the story?\n",
            "INFO:     34.91.73.162:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "\n",
            "Assistant:\n",
            "রবীন্দ্রনাথ ঠাকুর\n",
            "\n",
            "Recent History:\n",
            "\n",
            "  1) Q: কাকে অনুপমের ভাগ্য দেবতা বলে উল্লেখ করা হয়েছে?\n",
            "     A: মামাকে\n",
            "\n",
            "  2) Q: বিয়ের সময় কল্যাণীর প্রকৃত বয়স কত ছিল?\n",
            "     A: ১৬ বছর\n",
            "\n",
            "  3) Q:  who is the writer of the story?\n",
            "     A: রবীন্দ্রনাথ ঠাকুর\n",
            "\n",
            "============================================================\n",
            "\n",
            "You: when was the writer was born?\n",
            "INFO:     34.91.73.162:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "\n",
            "Assistant:\n",
            "মে ৭, ১৮৬১\n",
            "\n",
            "Recent History:\n",
            "\n",
            "  1) Q: বিয়ের সময় কল্যাণীর প্রকৃত বয়স কত ছিল?\n",
            "     A: ১৬ বছর\n",
            "\n",
            "  2) Q:  who is the writer of the story?\n",
            "     A: রবীন্দ্রনাথ ঠাকুর\n",
            "\n",
            "  3) Q: when was the writer was born?\n",
            "     A: মে ৭, ১৮৬১\n",
            "\n",
            "============================================================\n",
            "\n",
            "You:  father name of kollani?\n",
            "INFO:     34.91.73.162:0 - \"POST /ask HTTP/1.1\" 200 OK\n",
            "\n",
            "Assistant:\n",
            "শস্তুনাথ সেন\n",
            "\n",
            "Recent History:\n",
            "\n",
            "  1) Q:  who is the writer of the story?\n",
            "     A: রবীন্দ্রনাথ ঠাকুর\n",
            "\n",
            "  2) Q: when was the writer was born?\n",
            "     A: মে ৭, ১৮৬১\n",
            "\n",
            "  3) Q:  father name of kollani?\n",
            "     A: শস্তুনাথ সেন\n",
            "\n",
            "============================================================\n",
            "\n",
            "You: exit\n",
            "Session ended.\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "import requests\n",
        "\n",
        "def chat():\n",
        "    session_id = input(\"Enter session ID (press Enter for default): \") or \"default\"\n",
        "    url = f\"{public_url}/ask\"\n",
        "    clear_output()\n",
        "\n",
        "    print(f\"Chat session started: {session_id}\")\n",
        "    print(\"Type 'exit' to quit\\n\")\n",
        "\n",
        "    while True:\n",
        "        query = input(\"You: \")\n",
        "        if query.lower() in ['exit', 'quit']:\n",
        "            print(\"Session ended.\")\n",
        "            break\n",
        "\n",
        "        response = requests.post(\n",
        "            url,\n",
        "            json={\"query\": query, \"session_id\": session_id}\n",
        "        ).json()\n",
        "\n",
        "        print(f\"\\nAssistant:\\n{response['answer']}\\n\")\n",
        "\n",
        "        history = response.get(\"history\", [])\n",
        "        if history:\n",
        "            print(\"Recent History:\")\n",
        "            for i, exchange in enumerate(history[-3:], 1):\n",
        "                print(f\"\\n  {i}) Q: {exchange['query']}\")\n",
        "                print(f\"     A: {exchange['answer']}\")\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
        "\n",
        "chat()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
